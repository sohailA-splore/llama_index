{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7e0d5b7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/FaissIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
   "metadata": {},
   "source": [
    "# Vespa Vector Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "380a9254",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5580100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-vespa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375ec23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.20-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl.metadata (695 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.20 (from llama-index)\n",
      "  Downloading llama_index_core-0.10.20.post2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl.metadata (654 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.12-py3-none-any.whl.metadata (565 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.1.11-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading SQLAlchemy-2.0.28-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (3.8.6)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.12.2)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl.metadata (762 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (1.26.3)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading openai-1.14.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pandas in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (2.2.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (4.9.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading PyMuPDF-1.23.26-cp39-none-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_parse-0.3.9-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/syedsohailalialvi/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m840.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: click in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.12.25)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-macosx_10_9_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.20->llama-index) (2.1.0)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached greenlet-3.0.3.tar.gz (182 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.20->llama-index) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.20->llama-index) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
      "  Downloading pydantic_core-2.16.3-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/syedsohailalialvi/miniconda3/envs/aisg/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.10.20-py3-none-any.whl (5.6 kB)\n",
      "Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.9-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_core-0.10.20.post2-py3-none-any.whl (15.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.12-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.11-py3-none-any.whl (36 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_parse-0.3.9-py3-none-any.whl (6.8 kB)\n",
      "Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDF-1.23.26-cp39-none-macosx_10_9_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.22-py3-none-macosx_10_9_x86_64.whl (30.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.28-cp39-cp39-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.6.0-cp39-cp39-macosx_10_9_x86_64.whl (976 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m976.7/976.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp39-cp39-macosx_10_9_x86_64.whl (37 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: greenlet\n",
      "  Building wheel for greenlet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for greenlet: filename=greenlet-3.0.3-cp39-cp39-macosx_10_9_x86_64.whl size=213283 sha256=30efb65af63012c61ec0c9d4ef33b82f7da90c42be0af5fb71b8bcb3874e9e2f\n",
      "  Stored in directory: /Users/syedsohailalialvi/Library/Caches/pip/wheels/4e/b7/50/fef77b0dafc995530497e09a5123f00b1e68c9d305e9cf37e5\n",
      "Successfully built greenlet\n",
      "Installing collected packages: striprtf, dirtyjson, wrapt, pypdf, PyMuPDFb, pydantic-core, mypy-extensions, h11, greenlet, distro, annotated-types, typing-inspect, tiktoken, SQLAlchemy, pymupdf, pydantic, httpcore, deprecated, bs4, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed PyMuPDFb-1.23.22 SQLAlchemy-2.0.28 annotated-types-0.6.0 bs4-0.0.2 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-0.10.20 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.9 llama-index-core-0.10.20.post2 llama-index-embeddings-openai-0.1.6 llama-index-indices-managed-llama-cloud-0.1.4 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.12 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.11 llama-index-readers-llama-parse-0.1.3 llama-parse-0.3.9 llamaindex-py-client-0.1.13 mypy-extensions-1.0.0 openai-1.14.1 pydantic-2.6.4 pydantic-core-2.16.3 pymupdf-1.23.26 pypdf-4.1.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "#### Creating a Vespa Application Deployment\n",
    "\n",
    "From [scaling-personal-ai-assistants-with-streaming-mode-cloud.ipynb](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b5e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field, FieldSet, HNSW\n",
    "mail_schema = Schema(\n",
    "            name=\"mail\",\n",
    "            mode=\"streaming\",\n",
    "            document=Document(\n",
    "                fields=[\n",
    "                    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"subject\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"to\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"from\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"body\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"display_date\", type=\"string\", indexing=[\"summary\"]),\n",
    "                    Field(name=\"timestamp\", type=\"long\", indexing=[\"input display_date\", \"to_epoch_second\", \"summary\", \"attribute\"], is_document_field=False),\n",
    "                    Field(name=\"embedding\", type=\"tensor<bfloat16>(x[384])\",\n",
    "                        indexing=[\"\\\"passage: \\\" . input subject .\\\" \\\". input body\", \"embed e5\", \"attribute\", \"index\"],\n",
    "                        ann=HNSW(distance_metric=\"angular\"),\n",
    "                        is_document_field=False\n",
    "                    )\n",
    "                ],\n",
    "            ),\n",
    "            fieldsets=[\n",
    "                FieldSet(name = \"default\", fields = [\"subject\", \"body\", \"to\", \"from\"])\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3820b57-b952-491c-8a84-a98161f59bcb",
   "metadata": {},
   "source": [
    "In the mail schema, we have six document fields; these are provided by us when we feed documents of type mail to this app. The fieldset defines which fields are matched against when we do not mention explicit field names when querying. We can add as many fieldsets as we like without duplicating content.\n",
    "\n",
    "In addition to the fields within the document, there are two synthetic fields in the schema, timestamp and embedding, using Vespa indexing expressions taking inputs from the document and performing conversions.\n",
    "\n",
    "the timestamp field takes the input display_date and uses the to_epoch_second converter to convert the display date into an epoch timestamp. This is useful because we can calculate the document's age and use the freshness(timestamp) rank feature during ranking phases.\n",
    "the embedding tensor field takes the subject and body as input and feeds that into an embed function that uses an embedding model to map the string input into an embedding vector representation using 384 dimensions with bfloat16 precision. Vectors in Vespa are represented as Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dadf32-2d19-4639-a404-b5dbd68b51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field, FieldSet, HNSW\n",
    "calendar_schema = Schema(\n",
    "            name=\"calendar\",\n",
    "            inherits=\"mail\",\n",
    "            mode=\"streaming\",\n",
    "            document=Document(inherits=\"mail\",\n",
    "                fields=[\n",
    "                    Field(name=\"duration\", type=\"int\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"guests\", type=\"array<string>\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"location\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"address\", type=\"string\", indexing=[\"summary\", \"index\"])\n",
    "                ]\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cee15-1458-4c50-9e67-c3e59e69de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Component, Parameter\n",
    "\n",
    "vespa_app_name = \"assistant\"\n",
    "vespa_application_package = ApplicationPackage(\n",
    "        name=vespa_app_name,\n",
    "        schema=[mail_schema, calendar_schema],\n",
    "        components=[Component(id=\"e5\", type=\"hugging-face-embedder\",\n",
    "            parameters=[\n",
    "                Parameter(\"transformer-model\", {\"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/simple-semantic-search/model/e5-small-v2-int8.onnx\"}),\n",
    "                Parameter(\"tokenizer-model\", {\"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/simple-semantic-search/model/tokenizer.json\"})\n",
    "            ]\n",
    "        )]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f23789-84f8-4a11-8d2a-8a901563dd68",
   "metadata": {},
   "source": [
    "In the last step, we configure ranking by adding rank-profile's to the mail schema.\n",
    "\n",
    "Vespa supports phased ranking and has a rich set of built-in rank-features.\n",
    "\n",
    "Users can also define custom functions with ranking expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f1859-4220-4ee7-ac91-bff9f6eddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function, GlobalPhaseRanking, FirstPhaseRanking\n",
    "\n",
    "keywords_and_freshness = RankProfile(\n",
    "    name=\"default\", \n",
    "    functions=[Function(\n",
    "        name=\"my_function\", expression=\"nativeRank(subject) + nativeRank(body) + freshness(timestamp)\"\n",
    "    )],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"my_function\",\n",
    "        rank_score_drop_limit=0.02\n",
    "    ),\n",
    "    match_features=[\"nativeRank(subject)\", \"nativeRank(body)\", \"my_function\", \"freshness(timestamp)\"],\n",
    ")\n",
    "\n",
    "semantic = RankProfile(\n",
    "    name=\"semantic\", \n",
    "    functions=[Function(\n",
    "        name=\"cosine\", expression=\"max(0,cos(distance(field, embedding)))\"\n",
    "    )],\n",
    "    inputs=[(\"query(q)\", \"tensor<float>(x[384])\"), (\"query(threshold)\",\"\", \"0.75\")],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"if(cosine > query(threshold), cosine, -1)\",\n",
    "        rank_score_drop_limit=0.1\n",
    "    ),\n",
    "    match_features=[\"cosine\", \"freshness(timestamp)\", \"distance(field, embedding)\", \"query(threshold)\"],\n",
    ")\n",
    "\n",
    "fusion = RankProfile(\n",
    "    name=\"fusion\",\n",
    "    inherits=\"semantic\",\n",
    "    functions=[\n",
    "        Function(\n",
    "            name=\"keywords_and_freshness\", expression=\" nativeRank(subject) + nativeRank(body) + freshness(timestamp)\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"semantic\", expression=\"cos(distance(field,embedding))\"\n",
    "        )\n",
    "\n",
    "    ],\n",
    "    inputs=[(\"query(q)\", \"tensor<float>(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"if(cosine > query(threshold), cosine, -1)\",\n",
    "        rank_score_drop_limit=0.1\n",
    "    ),\n",
    "    match_features=[\"nativeRank(subject)\", \"keywords_and_freshness\", \"freshness(timestamp)\", \"cosine\", \"query(threshold)\"],\n",
    "    global_phase=GlobalPhaseRanking(\n",
    "        rerank_count=1000,\n",
    "        expression=\"reciprocal_rank_fusion(semantic, keywords_and_freshness)\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d362c-e084-4a10-908f-d74cb9ad4983",
   "metadata": {},
   "source": [
    "The default rank profile defines a custom function my_function that computes a linear combination of three different features:\n",
    "\n",
    "nativeRank(subject) Is a text matching feature , scoped to the subject field.\n",
    "nativeRank(body) Same, but scoped to the body field.\n",
    "freshness(timestamp) This is a built-in rank-feature that returns a number that is close to 1 if the timestamp is recent compared to the current query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a74218-eace-4112-ba0f-72781f7371ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_schema.add_rank_profile(keywords_and_freshness)\n",
    "mail_schema.add_rank_profile(semantic)\n",
    "mail_schema.add_rank_profile(fusion)\n",
    "calendar_schema.add_rank_profile(keywords_and_freshness)\n",
    "calendar_schema.add_rank_profile(semantic)\n",
    "calendar_schema.add_rank_profile(fusion)\n",
    "\n",
    "\n",
    "\n",
    "application_directory=\"my-assistant-vespa-app\"\n",
    "vespa_application_package.to_files(application_directory)\n",
    "import os\n",
    "\n",
    "def print_files_in_directory(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            print(os.path.join(root, file))\n",
    "print_files_in_directory(application_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee3f95-15db-4a47-b754-745f15714ad6",
   "metadata": {},
   "source": [
    "Finally, we have our basic Vespa schema and application package.\n",
    "\n",
    "We can serialize the representation to application package files. This is handy when we want to start working with production deployments and when we want to manage the application with version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a55548-b251-45f6-a52d-64c39f1be9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "#### Load documents, build the VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9096dae7",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335923ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b372a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "#### Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35369eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99212d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"What did the author do after his time at Y Combinator?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a720ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
